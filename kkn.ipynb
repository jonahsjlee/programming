{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQ6GuwMl0oPEvv8BpYluk+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonahsjlee/programming/blob/main/kkn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q0"
      ],
      "metadata": {
        "id": "OXUps9cwPsDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the difference between regression and classification?\n",
        "- Regression is used for predicting quantities. Classification is for assigning items to categories.\n"
      ],
      "metadata": {
        "id": "od8OK_RP7jxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a confusion table? What does it help us understand about a model's performance?\n",
        "- A performance measurement tool used for classification models. It shows how well the model differentiates between classes and helps identify potential issues like bias toward one class or the need to balance false positives and false negatives in certain applications."
      ],
      "metadata": {
        "id": "-LqdPdVc7kwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does the SSE quantify about a particular model? - It quantifies the overall error or deviation of a modelâ€™s predicted values from the actual observed values."
      ],
      "metadata": {
        "id": "QUDBI6ab7knO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - What are overfitting and underfitting? - Overfitting occurs when a model is too complex and fits the training data too closely, capturing noise and random fluctuations rather than the true underlying patterns. Underfitting occurs when a model is too simple to capture the underlying patterns in the data, leading to poor performance on both the training data and unseen data."
      ],
      "metadata": {
        "id": "vtJ4uAKc7kZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why does splitting the data into training and testing sets, and choosing\n",
        " by evaluating accuracy or SSE on the test set, improve model performance?\n",
        " -  Splitting the data into training and testing sets and evaluating model performance on the test set helps improve a model's ability to generalize to unseen data"
      ],
      "metadata": {
        "id": "YyJIduSc7jKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With classification, we can report a class label as a prediction or a probability distribution over class labels. Please explain the strengths and weaknesses of each approach.\n",
        "- Class label predictions can be easily interpreted, but can be unclear how certain the prediction is if probabilities are not reported.  "
      ],
      "metadata": {
        "id": "YGAEBEYU708U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1"
      ],
      "metadata": {
        "id": "23Qsp76LQitH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load the ./data/USA_cars_datasets.csv. Keep the following variables and drop the rest: price, year, mileage. Are there any NA's to handle? Look at the head and dimensions of the data."
      ],
      "metadata": {
        "id": "yt5xho6RRjWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('./data/USA_cars_datasets.csv')\n",
        "df = df.loc[:,['price','year','mileage'] ]\n",
        "print(df.shape)\n",
        "print(df.describe())\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "R8dbA6TUUl0o",
        "outputId": "5047544d-97f0-4b0c-90ed-8c946ba8e848"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data/USA_cars_datasets.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7ff280754673>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/USA_cars_datasets.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mileage'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/USA_cars_datasets.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Maxmin normalize year and mileage."
      ],
      "metadata": {
        "id": "zj5Lyxe_UoWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maxmin(x):\n",
        "    u = (x-min(x))/(max(x)-min(x))\n",
        "    return u\n",
        "\n",
        "df['year'] = maxmin(df['year'])\n",
        "df['mileage'] = maxmin(df['mileage'])"
      ],
      "metadata": {
        "id": "C60sif3MVSx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Split the sample into ~80% for training and ~20% for evaluation."
      ],
      "metadata": {
        "id": "vraByq9SUuOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y = df['price']\n",
        "X = df.drop('price',axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=100)"
      ],
      "metadata": {
        "id": "LQD3fODwVUxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Use the\n",
        "NN algorithm and the training data to predict price using year and mileage for the test set for\n",
        "k = 3, 10, 25, 50, 100, 300. For each value of\n",
        ", compute the mean squared error and print a scatterplot showing the test value plotted against the predicted value. What patterns do you notice as you increase k?"
      ],
      "metadata": {
        "id": "vbLkqw9YUxm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for k in [3,10,25,50,100,300]:\n",
        "    model = KNeighborsRegressor(n_neighbors=k).fit(X_train,y_train)\n",
        "    y_hat = model.predict(X_test)\n",
        "    SSE = np.sum( (y_test-y_hat)**2 )\n",
        "    #\n",
        "    plot, axes = plt.subplots()\n",
        "    plt.scatter(y_test,y_hat)\n",
        "    plt.title('k: '+str(k)+', SSE: '+str(SSE))\n",
        "    axes.set_ylim(-1000, 62000)\n",
        "    axes.set_xlim(-1000, 62000)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "V0TTHj09Va1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Determine the optimal k\n",
        " for these data."
      ],
      "metadata": {
        "id": "K9fk-jH4U4bM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_bar = 200\n",
        "k_grid = np.arange(1,k_bar) # The range of k's to consider\n",
        "SSE = np.zeros(k_bar)\n",
        "\n",
        "for k in range(k_bar):\n",
        "    fitted_model = KNeighborsRegressor(n_neighbors=k+1).fit(X_train,y_train)\n",
        "    y_hat = fitted_model.predict(X_test) # Predict values for test set\n",
        "    SSE[k] = np.sum( (y_test-y_hat)**2 ) # Save the computed SSE\n",
        "\n",
        "SSE_min = np.min(SSE) # Lowest recorded SSE\n",
        "min_index = np.where(SSE==SSE_min) # Find the indices of y that equal the minimum\n",
        "k_star = k_grid[min_index] # Find the optimal value of k\n",
        "print(k_star)\n",
        "\n",
        "plt.plot(np.arange(0,k_bar),SSE) # Plot SSE by k\n",
        "plt.xlabel(\"k\")\n",
        "plt.title(\"optimal k:\"+str(k_star))\n",
        "plt.ylabel('SSE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X3mrnqhVVdV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Describe what happened in the plots of predicted versus actual prices as\n",
        "k varied, taking your answer into part 6 into account. (Hint: Use the words \"underfitting\" and \"overfitting\".)"
      ],
      "metadata": {
        "id": "9uSSF7paVD2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The optimal $k$ is around 77, so for 300, the model is probably overfitting, and for 3, 10, and 25, the model is underfitting. For $k$ equal to 50 and 100, the answer is pretty close. What do we notice about $k=300$? There is a bunch of horizontal bunching, where common answers start to exert a lot of influence on the predictions and organize them into horizontal groups. Since the data are fairly evenly distributed, this is an unnatural artifact of over-fitting. On the other hand, for very small $k$, like 3, the predictions are high variance and low precision.\n",
        "\n",
        "- The other thing to notice is that as $k$ increases, the range of predictions shrinks: The high, outlier values become less influential, and predictions shrink towards average values."
      ],
      "metadata": {
        "id": "saKFN-kqVggl"
      }
    }
  ]
}